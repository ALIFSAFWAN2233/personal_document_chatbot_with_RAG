{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRYING/TESTING necessary libraries for document processing #\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1: Reading and extracting texts from various document formats ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolved Path: C:\\Users\\User\\Documents\\SideProject\\personal_doc_chatbot_with_RAG\\data\\raw_documents\\receipts\\train_ticket_receipt.pdf\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "file_path = Path(\"data/raw_documents/receipts/train_ticket_receipt.pdf\").resolve()\n",
    "print(\"Resolved Path:\", file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract from PDFs ####\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing c:/Users/User/Documents/SideProject/personal_document_chatbot_with_RAG/data/raw_documents/personal_documents/current_resume.pdf...\n",
      "[                                        ] (0/1=======================================[========================================] (1/1]\n",
      "# t s ters\n",
      "\n",
      "Current CGPA 3.49\n",
      "\n",
      "## PROFESSIONAL SUMMARY\n",
      "\n",
      "A final year student of Bachelor in Computer Science with the goal to seek and expand knowledge and\n",
      "specialize in Software Engineering field. With a strong passion for software engineering, I am actively\n",
      "seeking internship opportunities starting in September 2024 for 3 to 6 months. I am highly committed and\n",
      "self-driven individual to continuously improving skills relevant to the fast-paced technology industry.\n",
      "\n",
      "## 2 semester programme\n",
      "\n",
      "\n",
      "Bachelor of Computer Science with Honours\n",
      "Universiti Teknologi MARA Kampus Tapah\n",
      "\n",
      "Developed a web app that integrates custom trained model to recognize major guitar chord using visual data.\n",
      "Utilized Tensorflow and Keras framework for model development\n",
      "\n",
      "Module 3 Science Stream\n",
      "Kedah Matriculation College\n",
      "\n",
      "Collaborated and developed machine learning models to suggest optimal conditions for some aspects of farming.\n",
      "Utilized SVM, KNN, Decision Tree, and Streamlit for this assignment project.\n",
      "\n",
      "## PROJECTS\n",
      "\n",
      "Guitar Chord Recognition\n",
      "Final Year Project\n",
      "\n",
      "Developed a web app that integrates custom trained model to recognize major\n",
      "guitar chord using visual data.\n",
      "Utilized Tensorflow and Keras framework for model development\n",
      "Utilized Tailwind, Flask, MongoDB technologies to build the web app.\n",
      "\n",
      "FarmAi: Integrating Machine Learning for Sustainable Farming Solution\n",
      "\n",
      "Collaborated and developed machine learning models to suggest optimal\n",
      "conditions for some aspects of farming.\n",
      "Git\n",
      "\n",
      "CompQuizizz Mobile Application\n",
      "\n",
      "Collaborated and developed a quiz app for Computer Science students.\n",
      "Utilized Java programming language.\n",
      "\n",
      "## CERTIFICATIONS SKILLS\n",
      "\n",
      "CGPA 3.46 Git\n",
      "freeCodeCamp Machine Learning\n",
      "September 2021 Computer Vision\n",
      "\n",
      "Mobile Development Workshop: Flutter Python\n",
      "\n",
      "Flask\n",
      "\n",
      "UITM Kampus Kuala Terengganu\n",
      "\n",
      "MongoDB\n",
      "\n",
      "October 2023\n",
      "\n",
      "Fundamental of Cloud Computing\n",
      "Runcloud Education\n",
      "August 2022\n",
      "\n",
      "\n",
      "Utilized Tailwind, Flask, MongoDB technologies to build the web app.\n",
      "\n",
      "August 2020 - May 2021\n",
      "\n",
      "March 2024 - Present\n",
      "\n",
      "Machine Learning\n",
      "\n",
      "Dec 2023 - Jan 2024\n",
      "\n",
      "Computer Vision\n",
      "Python\n",
      "Flask\n",
      "MongoDB\n",
      "Intermediate\n",
      "\n",
      "|CERTIFICATIONS CGPA 3.46 freeCodeCamp September 2021 Mobile Development Workshop: Flutter UITM Kampus Kuala Terengganu October 2023 Fundamental of Cloud Computing Runcloud Education|SKILLS Git Computer Visin Machine Learning Python Computer Vision Flask Python MongoDB Flask Intermediate MongoDB Beginner|\n",
      "|---|---|\n",
      "\n",
      "\n",
      "-----\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reading and extracting texts from PDFs\n",
    "# by using PyMuPDF\n",
    "import pymupdf4llm\n",
    "\n",
    "#use ticket_train receipt as an example\n",
    "md_text = pymupdf4llm.to_markdown(\"c:/Users/User/Documents/SideProject/personal_document_chatbot_with_RAG/data/raw_documents/personal_documents/current_resume.pdf\")\n",
    "\n",
    "print(md_text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the extracted text into .md format file\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract from scanned PDFs/images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading and extracting texts from scanned PDFs\n",
    "# By using easyOCR (en,ms)\n",
    "import easyocr\n",
    "from pdf2image import convert_from_path\n",
    "reader = easyocr.Reader(['en','ms'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Page1\n",
      "KEMENTERIAN PENDIDIKAN MALAYSIA\n",
      "MINISTRY OF EDUCATION MALAYSIA\n",
      "LEMBAGA PEPERIKSAAN\n",
      "EXAMINATIONS SYNDICATE\n",
      "Calon yang namanya tercatat di bawah telah menduduki peperiksaan Sijil Pelajaran Malaysia (SPM) dan\n",
      "layak dianugerahi\n",
      "SIJIL PELAJARAN MALAYSIA\n",
      "MUHAMMAD ALIF SAFWAN BIN SYAMSYUL SYAHAR\n",
      "020630-02-0849\n",
      "KC208A089\n",
      "SMK AGAMA KEDAH\n",
      "Mata Pelajaran\n",
      "Gred\n",
      "Subject\n",
      "Grade\n",
      "BAHASA MELAYU\n",
      "A\n",
      "(CEMERLANG TINGGI)\n",
      "BAHASA INGGERIS\n",
      "A\n",
      "(CEMERLANG TINGGI)\n",
      "PENDIDIKAN ISLAM\n",
      "A+\n",
      "(CEMERLANG TERTINGGI)\n",
      "SEJARAH\n",
      "A+\n",
      "(CEMERLANG TERTINGGI)\n",
      "MATHEMATICS\n",
      "A\n",
      "(CEMERLANG TINGGI)\n",
      "BAHASA ARAB\n",
      "B+ (KEPUJIAN TERTINGGI)\n",
      "ADDITIONAL MATHEMATICS\n",
      "C\n",
      "(KEPUJIAN)\n",
      "PHYSICS\n",
      "A-\n",
      "(CEMERLANG)\n",
      "CHEMISTRY\n",
      "A\n",
      "(CEMERLANG TINGGI)\n",
      "BIOLOGY\n",
      "A+\n",
      "(CEMERLANG TERTINGGI)\n",
      "HIFZ AL-QURAN\n",
      "B+ (KEPUJIAN TERTINGGI)\n",
      "MAHARAT AL-QURAN\n",
      "B+ (KEPUJIAN TERTINGGI)\n",
      "JUMLAH MATA PELAJARAN DUA BELAS\n",
      "PEPERIKSAAN TAHUN 2019\n",
      "Pengarah Peperiksaan\n",
      "Director of Examinations\n",
      "191258556\n",
      "Kementerian Pendidikan Malaysia\n",
      "A\n",
      "04845697\n",
      "Ministry of Education Malaysia\n",
      "2423\n",
      "iaur\n",
      "SebTaheh MuTu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "poppler_path = r\"C:/Users/User/poppler-24.08.0/Library/bin\"\n",
    "pdf_path = \"C:/Users/User/Documents/SideProject/personal_document_chatbot_with_RAG/data/raw_documents/personal_documents/spm_certificate.pdf\"\n",
    "import numpy as np\n",
    "images = convert_from_path(pdf_path,poppler_path=poppler_path)\n",
    "\n",
    "all_text = []\n",
    "# Process each page image directly\n",
    "for i, img in enumerate(images):\n",
    "    # Convert PIL image to NumPy array\n",
    "    img_np = np.array(img)\n",
    "\n",
    "    #read the text from image\n",
    "    text = reader.readtext(img_np, detail=0) #extracts the text \n",
    "    page_text = \"\\n\".join(text) #joins the text lines\n",
    "    all_text.append(f\"## Page{i+1}\\n{page_text}\") #format each page by including a title/number of pages\n",
    "\n",
    "markdown_text = \"\\n\\n\".join(all_text)\n",
    "print(markdown_text)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the extracted text into .md format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading and extracting texts from .docx/.docs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2: Chunking the extracted text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement hybrid chunking (Split based on the headings of the markdown + fixed-length split )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1>t s ters</h1>\n",
      "<p>Current CGPA 3.49</p>\n",
      "<h2>PROFESSIONAL SUMMARY</h2>\n",
      "<p>A final year student of Bachelor in Computer Science with the goal to seek and expand knowledge and\n",
      "specialize in Software Engineering field. With a strong passion for software engineering, I am actively\n",
      "seeking internship opportunities starting in September 2024 for 3 to 6 months. I am highly committed and\n",
      "self-driven individual to continuously improving skills relevant to the fast-paced technology industry.</p>\n",
      "<h2>2 semester programme</h2>\n",
      "<p>Bachelor of Computer Science with Honours\n",
      "Universiti Teknologi MARA Kampus Tapah</p>\n",
      "<p>Developed a web app that integrates custom trained model to recognize major guitar chord using visual data.\n",
      "Utilized Tensorflow and Keras framework for model development</p>\n",
      "<p>Module 3 Science Stream\n",
      "Kedah Matriculation College</p>\n",
      "<p>Collaborated and developed machine learning models to suggest optimal conditions for some aspects of farming.\n",
      "Utilized SVM, KNN, Decision Tree, and Streamlit for this assignment project.</p>\n",
      "<h2>PROJECTS</h2>\n",
      "<p>Guitar Chord Recognition\n",
      "Final Year Project</p>\n",
      "<p>Developed a web app that integrates custom trained model to recognize major\n",
      "guitar chord using visual data.\n",
      "Utilized Tensorflow and Keras framework for model development\n",
      "Utilized Tailwind, Flask, MongoDB technologies to build the web app.</p>\n",
      "<p>FarmAi: Integrating Machine Learning for Sustainable Farming Solution</p>\n",
      "<p>Collaborated and developed machine learning models to suggest optimal\n",
      "conditions for some aspects of farming.\n",
      "Git</p>\n",
      "<p>CompQuizizz Mobile Application</p>\n",
      "<p>Collaborated and developed a quiz app for Computer Science students.\n",
      "Utilized Java programming language.</p>\n",
      "<h2>CERTIFICATIONS SKILLS</h2>\n",
      "<p>CGPA 3.46 Git\n",
      "freeCodeCamp Machine Learning\n",
      "September 2021 Computer Vision</p>\n",
      "<p>Mobile Development Workshop: Flutter Python</p>\n",
      "<p>Flask</p>\n",
      "<p>UITM Kampus Kuala Terengganu</p>\n",
      "<p>MongoDB</p>\n",
      "<p>October 2023</p>\n",
      "<p>Fundamental of Cloud Computing\n",
      "Runcloud Education\n",
      "August 2022</p>\n",
      "<p>Utilized Tailwind, Flask, MongoDB technologies to build the web app.</p>\n",
      "<p>August 2020 - May 2021</p>\n",
      "<p>March 2024 - Present</p>\n",
      "<p>Machine Learning</p>\n",
      "<p>Dec 2023 - Jan 2024</p>\n",
      "<p>Computer Vision\n",
      "Python\n",
      "Flask\n",
      "MongoDB\n",
      "Intermediate</p>\n",
      "<p>|CERTIFICATIONS CGPA 3.46 freeCodeCamp September 2021 Mobile Development Workshop: Flutter UITM Kampus Kuala Terengganu October 2023 Fundamental of Cloud Computing Runcloud Education|SKILLS Git Computer Visin Machine Learning Python Computer Vision Flask Python MongoDB Flask Intermediate MongoDB Beginner|\n",
      "|---|---|</p>\n",
      "<hr />\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use the md_text as an example\n",
    "\n",
    "from markdown_it import MarkdownIt\n",
    "\n",
    "md = MarkdownIt() #initialize the library\n",
    "print(md.render(md_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'# t s ters': 'Current CGPA 3.49', '## PROFESSIONAL SUMMARY': 'A final year student of Bachelor in Computer Science with the goal to seek and expand knowledge and\\nspecialize in Software Engineering field. With a strong passion for software engineering, I am actively\\nseeking internship opportunities starting in September 2024 for 3 to 6 months. I am highly committed and\\nself-driven individual to continuously improving skills relevant to the fast-paced technology industry.', '## 2 semester programme': 'Bachelor of Computer Science with Honours\\nUniversiti Teknologi MARA Kampus Tapah\\nDeveloped a web app that integrates custom trained model to recognize major guitar chord using visual data.\\nUtilized Tensorflow and Keras framework for model development\\nModule 3 Science Stream\\nKedah Matriculation College\\nCollaborated and developed machine learning models to suggest optimal conditions for some aspects of farming.\\nUtilized SVM, KNN, Decision Tree, and Streamlit for this assignment project.', '## PROJECTS': 'Guitar Chord Recognition\\nFinal Year Project\\nDeveloped a web app that integrates custom trained model to recognize major\\nguitar chord using visual data.\\nUtilized Tensorflow and Keras framework for model development\\nUtilized Tailwind, Flask, MongoDB technologies to build the web app.\\nFarmAi: Integrating Machine Learning for Sustainable Farming Solution\\nCollaborated and developed machine learning models to suggest optimal\\nconditions for some aspects of farming.\\nGit\\nCompQuizizz Mobile Application\\nCollaborated and developed a quiz app for Computer Science students.\\nUtilized Java programming language.', '## CERTIFICATIONS SKILLS': 'CGPA 3.46 Git\\nfreeCodeCamp Machine Learning\\nSeptember 2021 Computer Vision\\nMobile Development Workshop: Flutter Python\\nFlask\\nUITM Kampus Kuala Terengganu\\nMongoDB\\nOctober 2023\\nFundamental of Cloud Computing\\nRuncloud Education\\nAugust 2022\\nUtilized Tailwind, Flask, MongoDB technologies to build the web app.\\nAugust 2020 - May 2021\\nMarch 2024 - Present\\nMachine Learning\\nDec 2023 - Jan 2024\\nComputer Vision\\nPython\\nFlask\\nMongoDB\\nIntermediate\\n|CERTIFICATIONS CGPA 3.46 freeCodeCamp September 2021 Mobile Development Workshop: Flutter UITM Kampus Kuala Terengganu October 2023 Fundamental of Cloud Computing Runcloud Education|SKILLS Git Computer Visin Machine Learning Python Computer Vision Flask Python MongoDB Flask Intermediate MongoDB Beginner|\\n|---|---|'}\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "def split_markdown_by_headings(md_text):\n",
    "    md = MarkdownIt()  # Initialize Markdown parser\n",
    "    tokens = md.parse(md_text)  # Parse Markdown\n",
    "    sections = defaultdict(list)  # Store content based on headings / split content based on headings\n",
    "    \n",
    "    current_heading = \"No Heading\"  # Default if no heading at start\n",
    "\n",
    "    for token in tokens:\n",
    "        if token.type == \"heading_open\":  # Detect heading (h1, h2, h3, etc.)\n",
    "            level = int(token.tag[1])  # Extract heading level (1, 2, 3)\n",
    "            next_token = tokens[tokens.index(token) + 1]  # Get heading text\n",
    "            if next_token.type == \"inline\":\n",
    "                current_heading = f\"{'#' * level} {next_token.content}\"  # Format heading\n",
    "        \n",
    "        elif token.type == \"paragraph_open\":  # Detect paragraphs\n",
    "            next_token = tokens[tokens.index(token) + 1]\n",
    "            if next_token.type == \"inline\":\n",
    "                sections[current_heading].append(next_token.content)  # Store content under heading\n",
    "\n",
    "    # Convert dictionary values to strings\n",
    "    return {key: \"\\n\".join(value) for key, value in sections.items()}\n",
    "\n",
    "\n",
    "split_sections = split_markdown_by_headings(md_text)\n",
    "print(split_sections)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('# t s ters', 'Current CGPA 3.49')])\n",
      "dict_items([('# t s ters', 'Current CGPA 3.49'), ('## PROFESSIONAL SUMMARY', 'A final year student of Bachelor in Computer Science with the goal to seek and expand knowledge and\\nspecialize in Software Engineering field. With a strong passion for software engineering, I am actively\\nseeking internship opportunities starting in September 2024 for 3 to 6 months. I am highly committed and\\nself-driven individual to continuously improving skills relevant to the fast-paced technology industry.')])\n",
      "dict_items([('# t s ters', 'Current CGPA 3.49'), ('## PROFESSIONAL SUMMARY', 'A final year student of Bachelor in Computer Science with the goal to seek and expand knowledge and\\nspecialize in Software Engineering field. With a strong passion for software engineering, I am actively\\nseeking internship opportunities starting in September 2024 for 3 to 6 months. I am highly committed and\\nself-driven individual to continuously improving skills relevant to the fast-paced technology industry.'), ('## 2 semester programme', 'Bachelor of Computer Science with Honours\\nUniversiti Teknologi MARA Kampus Tapah\\nDeveloped a web app that integrates custom trained model to recognize major guitar chord using visual data.\\nUtilized Tensorflow and Keras framework for model development\\nModule 3 Science Stream\\nKedah Matriculation College\\nCollaborated and developed machine learning models to suggest optimal conditions for some aspects of farming.\\nUtilized SVM, KNN, Decision Tree, and Streamlit for this assignment project.')])\n",
      "dict_items([('# t s ters', 'Current CGPA 3.49'), ('## PROFESSIONAL SUMMARY', 'A final year student of Bachelor in Computer Science with the goal to seek and expand knowledge and\\nspecialize in Software Engineering field. With a strong passion for software engineering, I am actively\\nseeking internship opportunities starting in September 2024 for 3 to 6 months. I am highly committed and\\nself-driven individual to continuously improving skills relevant to the fast-paced technology industry.'), ('## 2 semester programme', 'Bachelor of Computer Science with Honours\\nUniversiti Teknologi MARA Kampus Tapah\\nDeveloped a web app that integrates custom trained model to recognize major guitar chord using visual data.\\nUtilized Tensorflow and Keras framework for model development\\nModule 3 Science Stream\\nKedah Matriculation College\\nCollaborated and developed machine learning models to suggest optimal conditions for some aspects of farming.\\nUtilized SVM, KNN, Decision Tree, and Streamlit for this assignment project.'), ('## PROJECTS', 'Guitar Chord Recognition\\nFinal Year Project\\nDeveloped a web app that integrates custom trained model to recognize major\\nguitar chord using visual data.\\nUtilized Tensorflow and Keras framework for model development\\nUtilized Tailwind, Flask, MongoDB technologies to build the web app.\\nFarmAi: Integrating Machine Learning for Sustainable Farming Solution\\nCollaborated and developed machine learning models to suggest optimal\\nconditions for some aspects of farming.\\nGit\\nCompQuizizz Mobile Application\\nCollaborated and developed a quiz app for Computer Science students.\\nUtilized Java programming language.')])\n",
      "dict_items([('# t s ters', 'Current CGPA 3.49'), ('## PROFESSIONAL SUMMARY', 'A final year student of Bachelor in Computer Science with the goal to seek and expand knowledge and\\nspecialize in Software Engineering field. With a strong passion for software engineering, I am actively\\nseeking internship opportunities starting in September 2024 for 3 to 6 months. I am highly committed and\\nself-driven individual to continuously improving skills relevant to the fast-paced technology industry.'), ('## 2 semester programme', 'Bachelor of Computer Science with Honours\\nUniversiti Teknologi MARA Kampus Tapah\\nDeveloped a web app that integrates custom trained model to recognize major guitar chord using visual data.\\nUtilized Tensorflow and Keras framework for model development\\nModule 3 Science Stream\\nKedah Matriculation College\\nCollaborated and developed machine learning models to suggest optimal conditions for some aspects of farming.\\nUtilized SVM, KNN, Decision Tree, and Streamlit for this assignment project.'), ('## PROJECTS', 'Guitar Chord Recognition\\nFinal Year Project\\nDeveloped a web app that integrates custom trained model to recognize major\\nguitar chord using visual data.\\nUtilized Tensorflow and Keras framework for model development\\nUtilized Tailwind, Flask, MongoDB technologies to build the web app.\\nFarmAi: Integrating Machine Learning for Sustainable Farming Solution\\nCollaborated and developed machine learning models to suggest optimal\\nconditions for some aspects of farming.\\nGit\\nCompQuizizz Mobile Application\\nCollaborated and developed a quiz app for Computer Science students.\\nUtilized Java programming language.'), ('## CERTIFICATIONS SKILLS', 'CGPA 3.46 Git\\nfreeCodeCamp Machine Learning\\nSeptember 2021 Computer Vision\\nMobile Development Workshop: Flutter Python\\nFlask\\nUITM Kampus Kuala Terengganu\\nMongoDB\\nOctober 2023\\nFundamental of Cloud Computing\\nRuncloud Education\\nAugust 2022\\nUtilized Tailwind, Flask, MongoDB technologies to build the web app.\\nAugust 2020 - May 2021\\nMarch 2024 - Present\\nMachine Learning\\nDec 2023 - Jan 2024\\nComputer Vision\\nPython\\nFlask\\nMongoDB\\nIntermediate\\n|CERTIFICATIONS CGPA 3.46 freeCodeCamp September 2021 Mobile Development Workshop: Flutter UITM Kampus Kuala Terengganu October 2023 Fundamental of Cloud Computing Runcloud Education|SKILLS Git Computer Visin Machine Learning Python Computer Vision Flask Python MongoDB Flask Intermediate MongoDB Beginner|\\n|---|---|')])\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# for each sections, check if the tokens in a section is more than 512 tokens, divide the section into more subsetions by using NLTK\n",
    "\n",
    "# function to chunk the text if it has more than 512 tokens\n",
    "def chunk_text(text, max_tokens=512):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    chunks = []\n",
    "\n",
    "    for i in range(0, len(words), max_tokens):\n",
    "        chunk_words = words[i: i + max_tokens]\n",
    "        chunk_text = \" \".join(chunk_words)\n",
    "        chunks.append(chunk_text)\n",
    "    return chunks\n",
    "    \n",
    "new_sections = {}\n",
    "\n",
    "\n",
    "for heading, content in split_sections.items():\n",
    "    tokens = nltk.word_tokenize(content)  # Count tokens in the content\n",
    "    \n",
    "    if len(tokens) > 512:\n",
    "        print(f\"Chunking '{heading}' as it has {len(tokens)} tokens...\")\n",
    "        chunked_subsections = chunk_text(content)  # Split content into 512-token chunks\n",
    "        \n",
    "        # Store each chunk under new subsections\n",
    "        for i, chunk in enumerate(chunked_subsections, start=1):\n",
    "            new_heading = f\"{heading} (Part {i})\"  # Create subheading\n",
    "            new_sections[new_heading] = chunk\n",
    "    else:\n",
    "        new_sections[heading] = content  # Keep original section\n",
    "    \n",
    "\n",
    "    print(new_sections.items())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Save the chunked data into a json format file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New document chunks appended to processed_data.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pathlib import Path\n",
    "\n",
    "chunk_title = \"Resume\"\n",
    "source = \"current_resume\"\n",
    "# Path to the existing JSON file\n",
    "output_path = \"processed_data.json\"\n",
    "\n",
    "# Load existing data if file exists, otherwise start with an empty list\n",
    "if Path(output_path).exists():\n",
    "    with open(output_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        try:\n",
    "            existing_data = json.load(file)\n",
    "            if not isinstance(existing_data, list):  # Ensure it's a list\n",
    "                existing_data = []\n",
    "        except json.JSONDecodeError:  # Handle empty or corrupted JSON\n",
    "            existing_data = []\n",
    "else:\n",
    "    existing_data = []\n",
    "\n",
    "# Convert new chunks to JSON format with token count\n",
    "new_json_data = []\n",
    "for heading, content in new_sections.items():\n",
    "    token_count = len(word_tokenize(content))  # Count tokens\n",
    "    new_json_data.append({\n",
    "        \"chunk_id\": chunk_title,\n",
    "        \"heading\": heading,\n",
    "        \"content\": content,\n",
    "        \"source\": source,\n",
    "        \"tokens\": token_count\n",
    "    })\n",
    "    chunk_counter=chunk_counter+1\n",
    "\n",
    "# Append new data to existing data\n",
    "existing_data.extend(new_json_data)\n",
    "\n",
    "# Save the updated JSON back to the file\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(existing_data, file, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"New document chunks appended to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline to read pdf files in a folder and extract it's text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pymupdf\n",
    "import pymupdf4llm\n",
    "import easyocr\n",
    "from pdf2image import convert_from_path\n",
    "from markdown_it import MarkdownIt\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunking function definition (Part 1: Chunk based on the headings) \n",
    "def split_markdown_by_headings(md_text):\n",
    "    md = MarkdownIt()  # Initialize Markdown parser\n",
    "    tokens = md.parse(md_text)  # Parse Markdown\n",
    "    sections = defaultdict(list)  # Store content based on headings / split content based on headings\n",
    "    \n",
    "    current_heading = \"No Heading\"  # Default if no heading at start\n",
    "\n",
    "    for token in tokens:\n",
    "        if token.type == \"heading_open\":  # Detect heading (h1, h2, h3, etc.)\n",
    "            level = int(token.tag[1])  # Extract heading level (1, 2, 3)\n",
    "            next_token = tokens[tokens.index(token) + 1]  # Get heading text\n",
    "            if next_token.type == \"inline\":\n",
    "                current_heading = f\"{'#' * level} {next_token.content}\"  # Format heading\n",
    "        \n",
    "        elif token.type == \"paragraph_open\":  # Detect paragraphs\n",
    "            next_token = tokens[tokens.index(token) + 1]\n",
    "            if next_token.type == \"inline\":\n",
    "                sections[current_heading].append(next_token.content)  # Store content under heading\n",
    "\n",
    "    # Convert dictionary values to strings\n",
    "    return {key: \"\\n\".join(value) for key, value in sections.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2: function to chunk the text if it has more than 512 tokens \n",
    "def chunk_text(text, max_tokens=512):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    chunks = []\n",
    "\n",
    "    for i in range(0, len(words), max_tokens):\n",
    "        chunk_words = words[i: i + max_tokens]\n",
    "        chunk_text = \" \".join(chunk_words)\n",
    "        chunks.append(chunk_text)\n",
    "    return chunks\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing C:/Users/User/Documents/SideProject/personal_document_chatbot_with_RAG/data/raw_documents/agreements\\ASB-ASB2-Application-Form-ENG-BM.pdf...\n",
      "[                                        ] (0/1=[==                                      ] ( 1/19=[====                                    ] ( 2/19=[======                                  ] ( 3/19=[========                                ] ( 4/19=[==========                              ] ( 5/19=[============                            ] ( 6/19=[==============                          ] ( 7/19=[================                        ] ( 8/19=[==================                      ] ( 9/19==[=====================                   ] (10/1=[=======================                 ] (11/1=[=========================               ] (12/1=[===========================             ] (13/1=[=============================           ] (14/1=[===============================         ] (15/1=[=================================       ] (16/1=[===================================     ] (17/1=[=====================================   ] (18/1=[========================================] (19/19]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'chunk_counter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 64\u001b[0m\n\u001b[0;32m     56\u001b[0m     token_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(word_tokenize(content))  \u001b[38;5;66;03m# Count tokens\u001b[39;00m\n\u001b[0;32m     57\u001b[0m     new_json_data\u001b[38;5;241m.\u001b[39mappend({\n\u001b[0;32m     58\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchunk_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: chunk_title,\n\u001b[0;32m     59\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheading\u001b[39m\u001b[38;5;124m\"\u001b[39m: heading,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: token_count\n\u001b[0;32m     63\u001b[0m     })\n\u001b[1;32m---> 64\u001b[0m     chunk_counter\u001b[38;5;241m=\u001b[39m\u001b[43mchunk_counter\u001b[49m\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m# Append new data to existing data\u001b[39;00m\n\u001b[0;32m     67\u001b[0m existing_data\u001b[38;5;241m.\u001b[39mextend(new_json_data)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'chunk_counter' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#Specify the folder \n",
    "folder_path = \"C:/Users/User/Documents/SideProject/personal_document_chatbot_with_RAG/data/raw_documents/agreements\"\n",
    "# Path to the existing JSON file\n",
    "output_path = \"C:/Users/User/Documents/SideProject/personal_document_chatbot_with_RAG/scripts/processed_data.json\"\n",
    "\n",
    "\n",
    "#Loop through all PDFs in the folder specified by folder_path\n",
    "for file in os.listdir(folder_path):\n",
    "    if file.endswith(\".pdf\"): #if the file has .pdf extension\n",
    "        pdf_path = os.path.join(folder_path, file) # create the full path for each pdf files\n",
    "\n",
    "        doc = pymupdf4llm.to_markdown(pdf_path) #extract the texts in the pdf into .md format\n",
    "\n",
    "        # Part 1 Chunking (Heading-based)\n",
    "        split_doc_sections = split_markdown_by_headings(doc)\n",
    "\n",
    "        # Part 2 Chunking (split further if tokens>512)\n",
    "\n",
    "        new_doc_sections = {} #new dict that stores much more splitted chunks\n",
    "\n",
    "        for heading, content in split_doc_sections.items():\n",
    "            tokens = nltk.word_tokenize(content)  # Count tokens in the content\n",
    "            \n",
    "            if len(tokens) > 512:\n",
    "                chunked_subsections = chunk_text(content)  # Split content into 512-token chunks\n",
    "                \n",
    "                # Store each chunk under new subsections\n",
    "                for i, chunk in enumerate(chunked_subsections, start=1):\n",
    "                    new_heading = f\"{heading} (Part {i})\"  # Create subheading\n",
    "                    new_doc_sections[new_heading] = chunk\n",
    "            else:\n",
    "                new_doc_sections[heading] = content  # Keep original section\n",
    "\n",
    "        # Append the new data chunks to the .json file\n",
    "        \n",
    "        chunk_title = os.path.splitext(os.path.basename(pdf_path))[0]\n",
    "        \n",
    "        source = os.path.splitext(os.path.basename(pdf_path))[0]\n",
    "\n",
    "\n",
    "        # Load existing data if file exists, otherwise start with an empty list\n",
    "        if Path(output_path).exists():\n",
    "            with open(output_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                try:\n",
    "                    existing_data = json.load(file)\n",
    "                    if not isinstance(existing_data, list):  # Ensure it's a list\n",
    "                        existing_data = []\n",
    "                except json.JSONDecodeError:  # Handle empty or corrupted JSON\n",
    "                    existing_data = []\n",
    "        else:\n",
    "            existing_data = []\n",
    "\n",
    "        # Convert new chunks to JSON format with token count\n",
    "        new_json_data = []\n",
    "        for heading, content in new_doc_sections.items():\n",
    "            token_count = len(word_tokenize(content))  # Count tokens\n",
    "            new_json_data.append({\n",
    "                \"chunk_id\": chunk_title,\n",
    "                \"heading\": heading,\n",
    "                \"content\": content,\n",
    "                \"source\": source,\n",
    "                \"tokens\": token_count\n",
    "            })\n",
    "\n",
    "\n",
    "        # Append new data to existing data\n",
    "        existing_data.extend(new_json_data)\n",
    "\n",
    "        # Save the updated JSON back to the file\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            json.dump(existing_data, file, indent=4, ensure_ascii=False)\n",
    "\n",
    "        print(f\"New document chunks appended to {output_path}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
