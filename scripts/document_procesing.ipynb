{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRYING/TESTING necessary libraries for document processing #\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1: Reading and extracting texts from various document formats ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolved Path: C:\\Users\\User\\Documents\\SideProject\\personal_doc_chatbot_with_RAG\\data\\raw_documents\\receipts\\train_ticket_receipt.pdf\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "file_path = Path(\"data/raw_documents/receipts/train_ticket_receipt.pdf\").resolve()\n",
    "print(\"Resolved Path:\", file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract from PDFs ####\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing c:/Users/User/Documents/SideProject/personal_document_chatbot_with_RAG/data/raw_documents/receipts/train_ticket_receipt.pdf...\n",
      "[                                        ] (0/2===================[====================                    ] (1/2===================[========================================] (2/2]\n",
      "# TAPAH ROAD - ANAK BUKIT\n",
      "\n",
      "\n",
      "### Berlepas / Departure:\n",
      "\n",
      "\n",
      "### Tiba / Arrival:\n",
      "\n",
      "\n",
      "### 16/11/2023 12:46 PM 16/11/2023 03:57 PM\n",
      "\n",
      " MUHAMMAD ALIF SAFWAN BIN SYAMSYUL SYAHAR - Adult\n",
      "\n",
      " xxxxxxxx0849\n",
      "\n",
      "\n",
      "#### MYR 46.00/DebitCard\n",
      "\n",
      " K231189695108 ** T231175999163 (Single)\n",
      "\n",
      "\n",
      "#### Online\n",
      "\n",
      "\n",
      "Airasia Ride Promo Code : KTMRIDE\n",
      "Sila berada di stesen 30 minit sebelum tren berlepas. Pintu pelepasan akan ditutup 5 minit sebelum waktu berlepas.\n",
      "\n",
      "Tertakluk kepada Akta Pengangkutan Awam Darat 2010 dan  Syarat Pengangkutan Penumpang.\n",
      "\n",
      "KTMB CALL CENTER +603-2267 1200\n",
      "\n",
      "\n",
      "### 9420\n",
      "\n",
      " EG9420\n",
      "\n",
      "###### Coach/Seat\n",
      "\n",
      "## C/5C\n",
      "\n",
      "###### Gold\n",
      "\n",
      "\n",
      "-----\n",
      "\n",
      "-----\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reading and extracting texts from PDFs\n",
    "# by using PyMuPDF\n",
    "import pymupdf4llm\n",
    "\n",
    "#use ticket_train receipt as an example\n",
    "md_text = pymupdf4llm.to_markdown(\"c:/Users/User/Documents/SideProject/personal_document_chatbot_with_RAG/data/raw_documents/receipts/train_ticket_receipt.pdf\")\n",
    "\n",
    "print(md_text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the extracted text into .md format file\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract from scanned PDFs/images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading and extracting texts from scanned PDFs\n",
    "# By using easyOCR (en,ms)\n",
    "import easyocr\n",
    "from pdf2image import convert_from_path\n",
    "reader = easyocr.Reader(['en','ms'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Page1\n",
      "KEMENTERIAN PENDIDIKAN MALAYSIA\n",
      "MINISTRY OF EDUCATION MALAYSIA\n",
      "LEMBAGA PEPERIKSAAN\n",
      "EXAMINATIONS SYNDICATE\n",
      "Calon yang namanya tercatat di bawah telah menduduki peperiksaan Sijil Pelajaran Malaysia (SPM) dan\n",
      "layak dianugerahi\n",
      "SIJIL PELAJARAN MALAYSIA\n",
      "MUHAMMAD ALIF SAFWAN BIN SYAMSYUL SYAHAR\n",
      "020630-02-0849\n",
      "KC208A089\n",
      "SMK AGAMA KEDAH\n",
      "Mata Pelajaran\n",
      "Gred\n",
      "Subject\n",
      "Grade\n",
      "BAHASA MELAYU\n",
      "A\n",
      "(CEMERLANG TINGGI)\n",
      "BAHASA INGGERIS\n",
      "A\n",
      "(CEMERLANG TINGGI)\n",
      "PENDIDIKAN ISLAM\n",
      "A+\n",
      "(CEMERLANG TERTINGGI)\n",
      "SEJARAH\n",
      "A+\n",
      "(CEMERLANG TERTINGGI)\n",
      "MATHEMATICS\n",
      "A\n",
      "(CEMERLANG TINGGI)\n",
      "BAHASA ARAB\n",
      "B+ (KEPUJIAN TERTINGGI)\n",
      "ADDITIONAL MATHEMATICS\n",
      "C\n",
      "(KEPUJIAN)\n",
      "PHYSICS\n",
      "A-\n",
      "(CEMERLANG)\n",
      "CHEMISTRY\n",
      "A\n",
      "(CEMERLANG TINGGI)\n",
      "BIOLOGY\n",
      "A+\n",
      "(CEMERLANG TERTINGGI)\n",
      "HIFZ AL-QURAN\n",
      "B+ (KEPUJIAN TERTINGGI)\n",
      "MAHARAT AL-QURAN\n",
      "B+ (KEPUJIAN TERTINGGI)\n",
      "JUMLAH MATA PELAJARAN DUA BELAS\n",
      "PEPERIKSAAN TAHUN 2019\n",
      "Pengarah Peperiksaan\n",
      "Director of Examinations\n",
      "191258556\n",
      "Kementerian Pendidikan Malaysia\n",
      "A\n",
      "04845697\n",
      "Ministry of Education Malaysia\n",
      "2423\n",
      "iaur\n",
      "SebTaheh MuTu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "poppler_path = r\"C:/Users/User/poppler-24.08.0/Library/bin\"\n",
    "pdf_path = \"C:/Users/User/Documents/SideProject/personal_document_chatbot_with_RAG/data/raw_documents/personal_documents/spm_certificate.pdf\"\n",
    "import numpy as np\n",
    "images = convert_from_path(pdf_path,poppler_path=poppler_path)\n",
    "\n",
    "all_text = []\n",
    "# Process each page image directly\n",
    "for i, img in enumerate(images):\n",
    "    # Convert PIL image to NumPy array\n",
    "    img_np = np.array(img)\n",
    "\n",
    "    #read the text from image\n",
    "    text = reader.readtext(img_np, detail=0) #extracts the text \n",
    "    page_text = \"\\n\".join(text) #joins the text lines\n",
    "    all_text.append(f\"## Page{i+1}\\n{page_text}\") #format each page by including a title/number of pages\n",
    "\n",
    "markdown_text = \"\\n\\n\".join(all_text)\n",
    "print(markdown_text)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the extracted text into .md format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading and extracting texts from .docx/.docs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2: Chunking the extracted text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement hybrid chunking (Split based on the headings of the markdown + fixed-length split )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1>TAPAH ROAD - ANAK BUKIT</h1>\n",
      "<h3>Berlepas / Departure:</h3>\n",
      "<h3>Tiba / Arrival:</h3>\n",
      "<h3>16/11/2023 12:46 PM 16/11/2023 03:57 PM</h3>\n",
      "<p>MUHAMMAD ALIF SAFWAN BIN SYAMSYUL SYAHAR - Adult</p>\n",
      "<p>xxxxxxxx0849</p>\n",
      "<h4>MYR 46.00/DebitCard</h4>\n",
      "<p>K231189695108 ** T231175999163 (Single)</p>\n",
      "<h4>Online</h4>\n",
      "<p>Airasia Ride Promo Code : KTMRIDE\n",
      "Sila berada di stesen 30 minit sebelum tren berlepas. Pintu pelepasan akan ditutup 5 minit sebelum waktu berlepas.</p>\n",
      "<p>Tertakluk kepada Akta Pengangkutan Awam Darat 2010 dan  Syarat Pengangkutan Penumpang.</p>\n",
      "<p>KTMB CALL CENTER +603-2267 1200</p>\n",
      "<h3>9420</h3>\n",
      "<p>EG9420</p>\n",
      "<h6>Coach/Seat</h6>\n",
      "<h2>C/5C</h2>\n",
      "<h6>Gold</h6>\n",
      "<hr />\n",
      "<hr />\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use the md_text as an example\n",
    "\n",
    "from markdown_it import MarkdownIt\n",
    "\n",
    "md = MarkdownIt() #initialize the library\n",
    "print(md.render(md_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'### 16/11/2023 12:46 PM 16/11/2023 03:57 PM': 'MUHAMMAD ALIF SAFWAN BIN SYAMSYUL SYAHAR - Adult\\nxxxxxxxx0849', '#### MYR 46.00/DebitCard': 'K231189695108 ** T231175999163 (Single)', '#### Online': 'Airasia Ride Promo Code : KTMRIDE\\nSila berada di stesen 30 minit sebelum tren berlepas. Pintu pelepasan akan ditutup 5 minit sebelum waktu berlepas.\\nTertakluk kepada Akta Pengangkutan Awam Darat 2010 dan  Syarat Pengangkutan Penumpang.\\nKTMB CALL CENTER +603-2267 1200', '### 9420': 'EG9420'}\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "def split_markdown_by_headings(md_text):\n",
    "    md = MarkdownIt()  # Initialize Markdown parser\n",
    "    tokens = md.parse(md_text)  # Parse Markdown\n",
    "    sections = defaultdict(list)  # Store content based on headings / split content based on headings\n",
    "    \n",
    "    current_heading = \"No Heading\"  # Default if no heading at start\n",
    "\n",
    "    for token in tokens:\n",
    "        if token.type == \"heading_open\":  # Detect heading (h1, h2, h3, etc.)\n",
    "            level = int(token.tag[1])  # Extract heading level (1, 2, 3)\n",
    "            next_token = tokens[tokens.index(token) + 1]  # Get heading text\n",
    "            if next_token.type == \"inline\":\n",
    "                current_heading = f\"{'#' * level} {next_token.content}\"  # Format heading\n",
    "        \n",
    "        elif token.type == \"paragraph_open\":  # Detect paragraphs\n",
    "            next_token = tokens[tokens.index(token) + 1]\n",
    "            if next_token.type == \"inline\":\n",
    "                sections[current_heading].append(next_token.content)  # Store content under heading\n",
    "\n",
    "    # Convert dictionary values to strings\n",
    "    return {key: \"\\n\".join(value) for key, value in sections.items()}\n",
    "\n",
    "\n",
    "split_sections = split_markdown_by_headings(md_text)\n",
    "print(split_sections)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('### 16/11/2023 12:46 PM 16/11/2023 03:57 PM', 'MUHAMMAD ALIF SAFWAN BIN SYAMSYUL SYAHAR - Adult\\nxxxxxxxx0849')])\n",
      "dict_items([('### 16/11/2023 12:46 PM 16/11/2023 03:57 PM', 'MUHAMMAD ALIF SAFWAN BIN SYAMSYUL SYAHAR - Adult\\nxxxxxxxx0849'), ('#### MYR 46.00/DebitCard', 'K231189695108 ** T231175999163 (Single)')])\n",
      "dict_items([('### 16/11/2023 12:46 PM 16/11/2023 03:57 PM', 'MUHAMMAD ALIF SAFWAN BIN SYAMSYUL SYAHAR - Adult\\nxxxxxxxx0849'), ('#### MYR 46.00/DebitCard', 'K231189695108 ** T231175999163 (Single)'), ('#### Online', 'Airasia Ride Promo Code : KTMRIDE\\nSila berada di stesen 30 minit sebelum tren berlepas. Pintu pelepasan akan ditutup 5 minit sebelum waktu berlepas.\\nTertakluk kepada Akta Pengangkutan Awam Darat 2010 dan  Syarat Pengangkutan Penumpang.\\nKTMB CALL CENTER +603-2267 1200')])\n",
      "dict_items([('### 16/11/2023 12:46 PM 16/11/2023 03:57 PM', 'MUHAMMAD ALIF SAFWAN BIN SYAMSYUL SYAHAR - Adult\\nxxxxxxxx0849'), ('#### MYR 46.00/DebitCard', 'K231189695108 ** T231175999163 (Single)'), ('#### Online', 'Airasia Ride Promo Code : KTMRIDE\\nSila berada di stesen 30 minit sebelum tren berlepas. Pintu pelepasan akan ditutup 5 minit sebelum waktu berlepas.\\nTertakluk kepada Akta Pengangkutan Awam Darat 2010 dan  Syarat Pengangkutan Penumpang.\\nKTMB CALL CENTER +603-2267 1200'), ('### 9420', 'EG9420')])\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# for each sections, check if the tokens in a section is more than 512 tokens, divide the section into more subsetions by using NLTK\n",
    "\n",
    "# function to chunk the text if it has more than 512 tokens\n",
    "def chunk_text(text, max_tokens=512):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    chunks = []\n",
    "\n",
    "    for i in range(0, len(words), max_tokens):\n",
    "        chunk_words = words[i: i + max_tokens]\n",
    "        chunk_text = \" \".join(chunk_words)\n",
    "        chunks.append(chunk_text)\n",
    "    return chunks\n",
    "    \n",
    "new_sections = {}\n",
    "\n",
    "\n",
    "for heading, content in split_sections.items():\n",
    "    tokens = nltk.word_tokenize(content)  # Count tokens in the content\n",
    "    \n",
    "    if len(tokens) > 512:\n",
    "        print(f\"Chunking '{heading}' as it has {len(tokens)} tokens...\")\n",
    "        chunked_subsections = chunk_text(content)  # Split content into 512-token chunks\n",
    "        \n",
    "        # Store each chunk under new subsections\n",
    "        for i, chunk in enumerate(chunked_subsections, start=1):\n",
    "            new_heading = f\"{heading} (Part {i})\"  # Create subheading\n",
    "            new_sections[new_heading] = chunk\n",
    "    else:\n",
    "        new_sections[heading] = content  # Keep original section\n",
    "    \n",
    "\n",
    "    print(new_sections.items())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Save the chunked data into a json format file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New document chunks appended to processed_data.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pathlib import Path\n",
    "\n",
    "chunk_counter =1 \n",
    "source = \"train_ticket_receipt\"\n",
    "# Path to the existing JSON file\n",
    "output_path = \"processed_data.json\"\n",
    "\n",
    "# Load existing data if file exists, otherwise start with an empty list\n",
    "if Path(output_path).exists():\n",
    "    with open(output_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        try:\n",
    "            existing_data = json.load(file)\n",
    "            if not isinstance(existing_data, list):  # Ensure it's a list\n",
    "                existing_data = []\n",
    "        except json.JSONDecodeError:  # Handle empty or corrupted JSON\n",
    "            existing_data = []\n",
    "else:\n",
    "    existing_data = []\n",
    "\n",
    "# Convert new chunks to JSON format with token count\n",
    "new_json_data = []\n",
    "for heading, content in new_sections.items():\n",
    "    token_count = len(word_tokenize(content))  # Count tokens\n",
    "    new_json_data.append({\n",
    "        \"chunk_id\": chunk_counter,\n",
    "        \"heading\": heading,\n",
    "        \"content\": content,\n",
    "        \"source\": source,\n",
    "        \"tokens\": token_count\n",
    "    })\n",
    "    chunk_counter=chunk_counter+1\n",
    "\n",
    "# Append new data to existing data\n",
    "existing_data.extend(new_json_data)\n",
    "\n",
    "# Save the updated JSON back to the file\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(existing_data, file, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"New document chunks appended to {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
