{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of Retrieval and Generation workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: setup the LLM model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criteria\n",
    "\n",
    "LLM of choice:\n",
    "1. llama 3.1/3.2\n",
    "2. deepseek r1 distill qwen 7b/ llama 8b\n",
    "\n",
    "May need to deploy the LLM online:\n",
    "1. Runpod\n",
    "2. Replicate\n",
    "3. OpenRouter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import sentence_transformers\n",
    "import chromadb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the model directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Load model\n",
    "model_name = \"meta-llama/Llama-3.2-3B-Instruct\" #3B = 12GB\n",
    "#model_name = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "#model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\" #7B parameter of BF16 = 13GB RAM needed # FP32 = 26GB RAM needed\n",
    "\n",
    "# Check if a GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load model and tokenizer\n",
    "print(\"Loading model...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16).to(device)\n",
    "print(\"Model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the function to create a chat template for the LLM\n",
    "def chat(messages, max_new_tokens=256):\n",
    "\n",
    "    # Format the conversation history for llama\n",
    "    formatted_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "    # Tokenize \n",
    "    inputs = tokenizer(formatted_prompt,return_tensors=\"pt\").to(\"cuda\")\n",
    "    # generate response\n",
    "    outputs = model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
    "\n",
    "    #Decode and return only the new assistant response\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONVERSATION HISTORY\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful personal assistant that is responsible to summarize, search for specific details,and give helpful answers to the user based on given context.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Context:\\n\" + retrieved_chunks + \"\\n\\nUser Query:\\n\" + user_query},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 14 Feb 2025\n",
      "\n",
      "You are a helpful personal assistant who knows about your user's personal documents and are responsible to summarize the documents, search for specific details from specific documents, and give helpful answers to the user.user\n",
      "\n",
      "Who are you?assistant\n",
      "\n",
      "I'm an AI personal assistant designed to help you with information and tasks. I have been trained on a vast amount of text data, including your personal documents, to provide you with quick and accurate answers to your questions.\n",
      "\n",
      "I can summarize documents, search for specific details, and offer helpful suggestions based on the information I have access to. I'm here to make your life easier and more productive.\n",
      "\n",
      "To get started, what would you like to do? Do you have a specific document you'd like me to summarize or search for information in?\n"
     ]
    }
   ],
   "source": [
    "# Get chatbot response\n",
    "response = chat(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the response generated:  What is LLM? Do not repeat the prompt in your response. Write your answer starting here [YOUR ANSWER HERE].\n",
      "\n",
      "\n",
      "[YOUR ANSWER HERE]\n",
      "\n",
      "Large Language Models (LLMs) are a type of artificial intelligence (AI) designed to process and understand human language. These models are trained on vast amounts of text data, allowing them to learn patterns, relationships, and structures within language. As a result, LLMs can generate human-like text, respond to questions, and even engage in conversation. They are commonly used in various applications, including language translation, text summarization, and content generation. LLMs have the potential to revolutionize the way we interact with technology and access information, but they also raise concerns about their limitations, biases, and potential misuse.\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the prompt and move tensors to GPU\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Generate response\n",
    "outputs = model.generate(inputs[\"input_ids\"],attention_mask=inputs[\"attention_mask\"], max_length=500 ,do_sample=True, pad_token_id=tokenizer.pad_token_id)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"This is the response generated: \", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example of user_query\n",
    "user_query = \"Who is Alif Safwan? \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ids': [['dc42bf3a-c3ba-4232-abb1-8d25de8f0039', '842bec56-8836-4ea6-bfa1-0c48bab26f1c']], 'embeddings': None, 'documents': [['MUHAMMAD ALIF SAFWAN BIN SYAMSYUL SYAHAR - Adult\\nxxxxxxxx0849', '**Referred by/Dirujuk oleh** **Sale Staff/Staf Jualan**\\nStaff Name/Nama Staf: Staff Name/Nama Staf:\\nStaff ID/ID Staf: Staff ID/ID Staf:\\nBranch/Cawangan: Branch/Cawangan:\\nBranch Code/Kod Cawangan: Branch Code/Kod Cawangan:\\nDate/Tarikh: Date/Tarikh:']], 'uris': None, 'data': None, 'metadatas': None, 'distances': None, 'included': [<IncludeEnum.documents: 'documents'>]}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model = sentence_transformers.SentenceTransformer(\"mixedbread-ai/mxbai-embed-large-v1\")\n",
    "#  Encode the query\n",
    "query =  user_query\n",
    "query_embedding = model.encode(query)\n",
    "\n",
    "# Search the vector database \n",
    "client = chromadb.PersistentClient(path=\"C:/Users/User/Documents/SideProject/personal_document_chatbot_with_RAG/data/vectorDB\")\n",
    "collection = client.get_collection(name=\"document_collection\")\n",
    "vector_response = collection.query(\n",
    "    query_embeddings= query_embedding.tolist(),\n",
    "    n_results=2,\n",
    "    include = [\"documents\"]\n",
    ")\n",
    "print(vector_response)\n",
    "\n",
    "# give the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Send the Retrieval information and Generate Response by using LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MUHAMMAD ALIF SAFWAN BIN SYAMSYUL SYAHAR - Adult\n",
      "xxxxxxxx0849**Referred by/Dirujuk oleh** **Sale Staff/Staf Jualan**\n",
      "Staff Name/Nama Staf: Staff Name/Nama Staf:\n",
      "Staff ID/ID Staf: Staff ID/ID Staf:\n",
      "Branch/Cawangan: Branch/Cawangan:\n",
      "Branch Code/Kod Cawangan: Branch Code/Kod Cawangan:\n",
      "Date/Tarikh: Date/Tarikh:\n"
     ]
    }
   ],
   "source": [
    "for k, v in vector_response.items():\n",
    "    if k == 'documents':\n",
    "        for i in v:\n",
    "            c=0\n",
    "            retrieved_chunks = i[0] + i[1]\n",
    "            print(retrieved_chunks)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# message template\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful personal assistant that is responsible to summarize, search for specific details,and give helpful answers to the user based on given context. If the answer is not clear within the given context, please elaborate wisely based on your current knowledge.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Context:\\n\" + retrieved_chunks + \"\\n\\nUser Query:\\n\" + user_query},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 14 Feb 2025\n",
      "\n",
      "You are a helpful personal assistant that is responsible to summarize, search for specific details,and give helpful answers to the user based on given context. If the answer is not clear within the given context, please elaborate wisely based on your current knowledge.user\n",
      "\n",
      "Context:\n",
      "MUHAMMAD ALIF SAFWAN BIN SYAMSYUL SYAHAR - Adult\n",
      "xxxxxxxx0849**Referred by/Dirujuk oleh** **Sale Staff/Staf Jualan**\n",
      "Staff Name/Nama Staf: Staff Name/Nama Staf:\n",
      "Staff ID/ID Staf: Staff ID/ID Staf:\n",
      "Branch/Cawangan: Branch/Cawangan:\n",
      "Branch Code/Kod Cawangan: Branch Code/Kod Cawangan:\n",
      "Date/Tarikh: Date/Tarikh:\n",
      "\n",
      "User Query:\n",
      "Who is Alif Safwan?assistant\n",
      "\n",
      "Tidak ada informasi yang cukup untuk memberikan jawaban yang spesifik tentang siapa Alif Safwan. Namun, saya dapat memberikan beberapa kemungkinan jawaban berdasarkan konteks yang diberikan.\n",
      "\n",
      "Alif Safwan adalah nama yang umum di beberapa negara, termasuk Indonesia. Namun, tanpa informasi lebih lanjut tentang konteks atau latar belakangnya, sulit untuk menentukan siapa Alif Safwan yang tepat.\n",
      "\n",
      "Jika Anda dapat memberikan informasi lebih lanjut tentang konteks atau latar belakang Alif Safwan, saya dapat membantu Anda menemukan jawaban yang lebih spesifik.\n"
     ]
    }
   ],
   "source": [
    "# Get chatbot response\n",
    "response = chat(messages)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
